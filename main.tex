\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{algorithm2e}
\usepackage[bottom]{footmisc}
\usepackage{SCITEPRESS}
\usepackage{float}

% --- ADDED PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{url}
% ---------------------------

\begin{document}

\sloppy % Ajuda a evitar que palavras longas estourem a margem

\title{Academic Dropout in Brazilian Universities: Development of Algorithms for Risk Profile Identification}

\author{\authorname{Vitória de Lourdes Carvalho Santos\sup{1}\orcidAuthor{0000-0000-0000-0000} and Wladmir Cardoso Brandao\sup{1}\orcidAuthor{0000-0000-0000-0000}}
\affiliation{\sup{1}Institute of Exact Sciences and Informatics, Pontifícia Universidade Católica de Minas Gerais (PUC Minas), Belo Horizonte, Brazil}
\email{vitoria.lourdes@sga.pucminas.br, wladmir@pucminas.br}
}

\keywords{School Dropout, Higher Education, Machine Learning, XGBoost, Predictive Models.}

\abstract{School dropout in higher education has a significant impact on students' professional development and on the efficiency of educational institutions. This study develops and compares three predictive models (Decision Tree, Neural Network, and XGBoost) to identify dropout risk profiles among university students at PUC Minas, based on a dataset of 94,052 records enriched with institutional course information. By applying data mining and machine learning techniques, the XGBoost model achieved 94.3\% accuracy and 98.0\% AUC-ROC, demonstrating superior performance in identifying at-risk students. The results support proactive retention strategies and validate the importance of academic integration factors predicted by Tinto's theoretical model.}

\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

Academic dropout in Brazilian higher education represents a serious challenge for both students and institutions. According to the \textit{Higher Education Map in Brazil -- Semesp} (2024), the average dropout rate in higher education is \textbf{57.2\%}, being more acute in private institutions (around \textbf{61\%}) and less prevalent in public institutions (around \textbf{40\%})~\cite{semesp2024}. Furthermore, the 2023 Higher Education Census indicates that only \textbf{27.1\%} of young people between 18 and 24 years old are enrolled in undergraduate courses, a figure still far from the goal of \textbf{33\%} stipulated by the National Education Plan~\cite{inep2023}.

Dropout implies a loss of human potential, waste of resources, and aggravation of social inequalities, since failing to complete higher education is associated with greater difficulties in entering the labor market~\cite{carvalho2021}. Dropout is a \textbf{multifactorial} phenomenon, involving academic, psychosocial, demographic, and socioeconomic aspects~\cite{rodrigues2022}.

In this context, the objective of this work is to develop \textbf{predictive} models, using \textbf{data mining} and \textbf{machine learning} techniques, capable of identifying dropout risk profiles among university students, supporting the adoption of proactive retention strategies.

\section{\uppercase{Literature Review}}

The remainder of this paper is organized as follows: Section \ref{sec:related_work} presents the literature review. Section \ref{sec:methodology} describes the methodology, including data enrichment and model construction. Section \ref{sec:results} discusses the results obtained with the predictive models. Finally, Section \ref{sec:conclusion} presents the conclusions and suggestions for future work.

\subsection{Academic Dropout: Concepts and Causes}

Dropout in higher education is recognized as a complex phenomenon. In Brazil, factors such as unsatisfactory performance in subjects, unfavorable socioeconomic conditions, financial default, and personal lack of motivation are recurrent. Psychosocial elements, such as a lack of integration with the academic community and dissatisfaction with the course, also contribute.

Among the main theoretical frameworks, the model of \textbf{academic and social integration} stands out \cite{tinto1975,tinto1987}, which understands dropout as the result of a progressive trajectory of institutional disengagement. Lower integration of the student with academic and social aspects increases the probability of dropout.

\subsection{Prediction Techniques Applied to Education}

The use of predictive analysis techniques is a promising alternative to anticipate risk behaviors, using historical data and institutional variables for the development of computational models with significant prediction capacity~\cite{pereira2023,rodrigues2022,carvalho2021}. Recent international literature reinforces that machine learning algorithms are superior to traditional statistical methods in the task of predicting academic performance~\cite{rastrollo2020}.

Machine learning algorithms such as the \textbf{Decision Tree} are valued for their interpretability~\cite{batista2021,carvalho2021}, while \textit{ensemble} methods like \textbf{XGBoost} demonstrate superior predictive capacity~\cite{zhao2023,putra2025}. \textbf{Neural Networks} also present promising results, especially with large volumes of data~\cite{sun2023}. Studies reinforce that the choice of algorithm must consider both \textbf{accuracy} and \textbf{interpretability} of results to support academic managers in decision-making~\cite{liu2022,putra2025}.

\subsection{Related Works and Identified Gaps}

Several studies seek to predict dropout with high precision, frequently reaching or exceeding \textbf{80\%} accuracy with \textit{ensemble} algorithms~\cite{zhao2023} and deep neural networks~\cite{sun2023}. In Brazil, the incorporation of academic variables (grades, attendance), demographic, and socioeconomic variables (scholarships, default) is consistently relevant to model precision~\cite{guimaraes2023,pereira2023,carvalho2021}.

\subsubsection{Methodological Differentiators of This Work}

To contextualize the contributions of this research, it is fundamental to highlight four methodological differentiators that distinguish this work from related literature:

\textbf{1. Institutional Data Enrichment}

While most studies use only isolated academic databases, this work implemented a \textbf{systematic enrichment} process by integrating Canvas data with \textbf{96\% coverage}. Studies such as Gašević et al.~\cite{gasevic2016} point out that ignoring instructional conditions and course context limits the precision of analyses, validating this approach. The four new institutional features (course modality, campus, course maturity, status) contributed \textbf{8.9\% of the total importance} in the XGBoost model, demonstrating that contextual factors of the institution significantly impact dropout risk.

\textbf{Measured Gain}: All three models showed consistent improvement of \textbf{+1.0\% to +2.5\%} in main metrics after enrichment (see Table~\ref{tab:comparacao_original_enriquecido}), highlighting Decision Tree Precision (+2.5\%) and Neural Network Recall (+2.2\%).

\textbf{2. Systematic Comparison: Original vs. Enriched}

Unlike works that present only final results, this study implemented a \textbf{rigorous scientific methodology} to validate the value of enrichment:
\begin{itemize}
    \item \textbf{Identical hyperparameters} between original and enriched versions of each model;
    \item \textbf{Same train/test split} (random\_\allowbreak state=42) for comparability;
    \item \textbf{Statistical analysis} of gains in all metrics.
\end{itemize}

This approach ensures that any observed improvement is \textbf{exclusively attributable to data enrichment}, eliminating methodological confounding variables.

\textbf{3. Scale and Diversity of the Dataset}

With \textbf{94,052 records} distributed across \textbf{522 courses} over \textbf{4 years}, this work operates on a significantly larger scale than most Brazilian studies, which are often limited to a few courses or short periods. This scale allows for greater \textbf{generalization} of models, capture of \textbf{temporal variations} (including pandemic impact), and \textbf{statistical robustness} in conclusions.

\textbf{4. Validation with Theoretical Model}

The feature importance analysis empirically validated Tinto's theoretical model: \textbf{Academic Integration} (Semesters Taken: 25.5\%, Grades: 8.7\%, Attendance: 4.7\%) represents \textbf{38.9\%} of total importance, converging theory and empirical data.

\subsubsection{Comparative Performance}

The XGBoost model achieved \textbf{94.3\% accuracy} and \textbf{98.0\% AUC-ROC}, positioning itself among the \textbf{best results reported} in Brazilian literature on dropout prediction. This performance is superior to most national studies reporting 80-90\% accuracy. The \textbf{Recall of 92\%} ensures that only 8\% of real dropout cases are not identified, minimizing losses of at-risk students.

\subsubsection{Identified Gaps}

Despite these advances, some important gaps remain: (i) lack of effective integration between data-based predictive models and theoretical dropout models, (ii) absence of systematic analyses of the impact of data enrichment with institutional variables, and (iii) disregard for \textbf{practical application} in academic management systems. This work seeks to address these gaps through a holistic approach combining methodological rigor, theoretical validation, and focus on institutional applicability.

\section{\uppercase{Methodology}}

The methodology adopted is divided into three main stages: (i) data preparation, enrichment, and exploratory analysis, (ii) construction of predictive models, and (iii) performance evaluation.

\subsection{Data Preparation and Enrichment}

The study uses an anonymized academic database from \textbf{PUC Minas} (\textbf{2020-2023}). The database contains \textbf{94,052 records} of students distributed across \textbf{522 courses}, with 36 original variables and a general dropout rate of \textbf{33.2\%} (31,252 dropouts and 62,800 non-dropouts).

The original variables cover four main dimensions:

{\raggedright
\textbf{(i) Academic Performance}: media\_\allowbreak nota\_\allowbreak anterior (average of grades obtained), media\_\allowbreak frequencia\_\allowbreak anterior (average attendance), qtd\_\allowbreak reprovacoes\_\allowbreak curso (total failures), qtd\_\allowbreak disc\_\allowbreak reprov\_\allowbreak nota\_\allowbreak curso and qtd\_\allowbreak disc\_\allowbreak reprov\_\allowbreak frequencia\_\allowbreak curso (subjects failed by grade and attendance);
\par}

{\raggedright
\textbf{(ii) Academic Situation}: qtd\_\allowbreak semestres\_\allowbreak cursados (time of permanence), perc\_\allowbreak cursado (percentage of course completed), dsc\_\allowbreak situacao\_\allowbreak aluno\_\allowbreak curso (current student status);
\par}

{\raggedright
\textbf{(iii) Personal and Demographic Attributes}: idade\_\allowbreak aluno (age), sexo\_\allowbreak aluno (sex), val\_\allowbreak distancia\_\allowbreak campus (distance between home and campus), dsc\_\allowbreak turno (study shift), dsc\_\allowbreak forma\_\allowbreak ingresso (form of admission);
\par}

{\raggedright
\textbf{(iv) Socioeconomic Factors}: ind\_\allowbreak possui\_\allowbreak bolsa (scholarship indicator), ind\_\allowbreak possui\_\allowbreak financiamento (student financing), ind\_\allowbreak inadimplente (financial default).
\par}

The preparation process involved:
\begin{itemize}
    \item \textbf{Cleaning}: Imputation of missing values (median/mode) and treatment of \textit{outliers} (IQR).
    \item \textbf{Encoding and Normalization}: \textit{One-Hot Encoding} for nominal variables and \textit{StandardScaler} for numerical variables.
    \item \textbf{Balancing}: Application of the \textbf{SMOTE} (\textit{Synthetic Minority Over-sampling Technique}) technique to mitigate class imbalance, according to literature recommendations for educational data~\cite{chawla2002}.
    \item \textbf{Exploratory Analysis (EDA)}: Used extensively to identify patterns in the complete database (Section 5.6).
\end{itemize}

\subsubsection{Institutional Data Enrichment}

A methodological differentiator of this work was the \textbf{enrichment} of the database through integration with institutional course information. The database used, containing \textbf{26,100 course records} from the Canvas platform, was related to the main database through course codes extracted by regular expressions from the sis\_\allowbreak source\_\allowbreak id field.

The enrichment process achieved \textbf{96\% coverage} (90,309 of 94,052 students), adding \textbf{7 new variables} to the original set:

\textbf{(i) Course Modality} (modalidade\_\allowbreak curso\_\allowbreak encoded): Classification extracted from the course name into Distance Learning (EAD), Online, On-campus, or Hybrid, allowing identification of risk differences between teaching modalities;

\textbf{(ii) Campus/Unit} (campus\_\allowbreak encoded): Identification of the teaching unit (PPL, PSG, PMG, PBE, etc.), capturing regional and infrastructure variations;

\textbf{(iii) Course Maturity} (dias\_\allowbreak desde\_\allowbreak criacao): Time in days since the course creation, reflecting institutional consolidation and curriculum;

\textbf{(iv) Course State} (curso\_\allowbreak ativo): Binary indicator if the course is active or has been discontinued (workflow\_\allowbreak state), relevant for identifying courses in the process of closing.

This enrichment expanded the set from \textbf{36 original variables} to \textbf{43 final features} (after encoding and derivation of variables), allowing the model to capture not only individual characteristics of students but also \textbf{institutional contextual factors} that impact dropout risk.

\subsection{Construction of Predictive Models}

Three classification models were implemented to predict dropout (ind\_\allowbreak evadido):
\begin{itemize}
    \item \textbf{Decision Tree}: High interpretability, useful for understanding classification criteria~\cite{batista2021,carvalho2021}.
    \item \textbf{XGBoost (Extreme Gradient Boosting)}: Optimized \textit{ensemble} algorithm, known for its \textbf{high predictive performance} on structured data~\cite{chen2016}.
    \item \textbf{Neural Network (MLP)}: Multi-Layer Perceptron (3 hidden layers), capable of modeling complex non-linear relationships~\cite{sun2023}.
\end{itemize}
The implementation used a stratified sample of \textbf{15,000 records} with hyperparameters adjusted for class balancing.

\subsection{Performance Evaluation}

The model was validated using the \textbf{train-test split} technique (\textbf{80\%} for training, \textbf{20\%} for testing) with stratification. The metrics used were Accuracy, Precision, Recall, F1-Score, and \textbf{AUC-ROC} (Area Under the ROC Curve), the latter being crucial for evaluating discriminatory capacity in a context of imbalanced classes.

\section{\uppercase{Results of Predictive Models}}

The three models were trained and tested in two configurations: (i) using only the \textbf{original data} (18 features from financeiro.csv) and (ii) using the \textbf{enriched data} (22 features, including 4 new institutional variables from accounts.csv). This approach allows for evaluating the impact of data enrichment on the predictive capacity of the models.

\subsection{Comparison: Original vs. Enriched Data}

Table~\ref{tab:comparacao_original_enriquecido} presents the complete comparative analysis of the three models in both data configurations, evidencing the gains obtained with institutional enrichment.

\begin{table}[H]
\centering
\caption{Performance Comparison: Original vs. Enriched Data.}
\label{tab:comparacao_original_enriquecido}
% Reduces space between columns to allow larger font
\setlength{\tabcolsep}{2.5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{llccccc}
\toprule
\textbf{Model} & \textbf{Data} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{AUC} \\
\midrule
\multirow{3}{*}{\shortstack[l]{Decision\\Tree}}
& Original & 86.8 & 76.5 & 86.1 & 81.1 & 93.8 \\
& Enriched & \textbf{88.1} & \textbf{79.0} & \textbf{88.0} & \textbf{83.0} & \textbf{94.8} \\
& \textit{Gain} & \textit{+1.3} & \textit{+2.5} & \textit{+1.9} & \textit{+1.9} & \textit{+1.0} \\
\midrule
\multirow{3}{*}{\shortstack[l]{Neural\\Network}}
& Original & 91.2 & 87.5 & 85.3 & 86.4 & 96.6 \\
& Enriched & \textbf{92.2} & \textbf{87.7} & \textbf{87.5} & \textbf{87.6} & \textbf{97.3} \\
& \textit{Gain} & \textit{+1.0} & \textit{+0.2} & \textit{+2.2} & \textit{+1.2} & \textit{+0.7} \\
\midrule
\multirow{3}{*}{XGBoost}
& Original & 93.2 & 89.0 & 90.3 & 89.7 & 98.0 \\
& Enriched & \textbf{94.3} & \textbf{91.0} & \textbf{92.0} & \textbf{92.0} & \textbf{98.0} \\
& \textit{Gain} & \textit{+1.1} & \textit{+2.0} & \textit{+1.7} & \textit{+2.3} & \textit{+0.0} \\
\bottomrule
\end{tabular}%
}
\end{table}

The magnitude of the data enrichment impact is detailed visually in Figure~\ref{fig:ganhos_enriquecimento}. The graph highlights that, although the accuracy gain seems marginal, the gains in sensitive metrics such as Recall and Precision were substantial, especially for the Decision Tree and the Neural Network.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{Grafico_Ganhos_Enriquecimento.png}
    \caption{Detail of percentage gains obtained by metric and model after data enrichment.}
    \label{fig:ganhos_enriquecimento}
\end{figure}

\textbf{Analysis of Results:}

Data enrichment with institutional variables demonstrated a \textbf{consistent positive impact} on all three models, with gains varying between \textbf{+0.7 and +2.5 percentage points} in the main metrics. Although the gains seem modest in absolute terms, they are \textbf{statistically significant} in a context of already high-performance prediction (>90\% accuracy).

\textbf{Analysis by Model:}

\begin{itemize}
    \item \textbf{Decision Tree}: Presented the highest absolute gains (\textbf{+1.3\%} accuracy, \textbf{+2.5\%} precision), suggesting that the new institutional features provided \textbf{more informative splitting criteria} for the decision rules. The 1.9 percentage point increase in Recall is particularly relevant, reducing false negatives (unidentified at-risk students).
    \item \textbf{Neural Network}: Registered moderate gain in accuracy (\textbf{+1.0\%}), but significantly improved \textbf{Recall (+2.2\%)}, indicating that the enriched features helped the network capture more subtle dropout patterns. The modest gain in precision (\textbf{+0.2\%}) suggests that the network was already modeling non-linear interactions well with the original data.
    \item \textbf{XGBoost}: Even being the best-performing model, it still showed relevant gains (\textbf{+1.1\%} accuracy, \textbf{+2.3\%} F1-Score), demonstrating that institutional enrichment adds value even for sophisticated \textit{ensemble} algorithms. The fact that AUC-ROC remained at \textbf{98.0\%} in both scenarios indicates that the original model already possessed excellent discriminatory capacity, but enrichment improved the calibration of predictions (simultaneously better precision and recall).
\end{itemize}

\textbf{Scientific Value of Enrichment:}

The results empirically demonstrate that \textbf{contextual institutional factors} (teaching modality, campus, course maturity) significantly complement individual factors (grades, attendance, failures). This evidence reinforces the need for \textbf{holistic} approaches in dropout prediction, integrating both student characteristics and the \textbf{institutional environment}.

The feature importance analysis of the enriched XGBoost (Section~\ref{sec:importancia_features}) revealed that \textbf{Course Modality} (4.2\%) and \textbf{Days Since Creation} (4.7\%) are among the 10 most important variables, validating the enrichment strategy. Distance learning courses, for example, presented systematically higher dropout rates (\textbf{50.9\%}), information that would not be captured by the original data.

\textbf{Practical Implications:}

For implementation in production systems, the use of \textbf{enriched models} is recommended, given that:
\begin{itemize}
    \item The gain of \textbf{1-2 percentage points} represents \textbf{dozens of students} saved from dropout in a large institution;
    \item The additional computational cost is \textbf{minimal} (same order of magnitude);
    \item Integration with institutional systems (Canvas LMS) is feasible and scalable;
    \item Enriched features provide \textbf{actionable insights} for managers (e.g., risk modalities, new courses need additional support).
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{Grafico_Acurácia.png}
    \caption{Visual Comparison of Accuracy: Of the Three Models with Original vs. Enriched Data.}
    \label{fig:comparacao_completa}
\end{figure}

\subsection{Detailed Performance of Decision Tree}

The \textbf{Decision Tree} was implemented as a \textit{baseline} model due to its high \textbf{interpretability} and ability to generate explicit decision rules. The model achieved \textbf{88.1\%} accuracy and \textbf{94.8\%} AUC-ROC, demonstrating satisfactory performance for identifying at-risk students.

\textbf{Hyperparameters Used:}
\begin{itemize}
    \item max\_\allowbreak depth=10: Maximum tree depth, controlling complexity;
    \item min\_\allowbreak samples\_\allowbreak split=100: Minimum samples for node splitting;
    \item class\_\allowbreak weight='balanced': Automatic class balancing;
    \item criterion='gini': Impurity measure for node splitting.
\end{itemize}

\textbf{Performance Analysis:}
The model presented \textbf{Recall of 88\%}, indicating that it correctly identifies the majority of dropout cases. The \textbf{Precision of 79\%} reveals that approximately 21\% of alerts are false positives, acceptable for an early warning system where it is preferable to investigate some unnecessary cases than to lose students at real risk.

Variable importance analysis revealed strong dominance of \textbf{Percentage Completed} (57.4\%) and \textbf{Semesters Taken} (12.3\%), confirming that academic progress is the main decision criterion. This simplicity makes the model easily explainable to academic managers, being useful for institutional communication of retention policies.

\textbf{Advantages}: Transparent decision rules, fast training and inference, low computational cost.

\textbf{Limitations}: Tendency to \textit{overfit} with complex data, lower capacity to capture non-linear interactions between multiple variables, inferior performance to more sophisticated models.

\subsection{Detailed Performance of Neural Network (MLP)}

The \textbf{Multi-Layer Perceptron (MLP) Neural Network} was implemented to capture \textbf{complex non-linear relationships} between features, reaching \textbf{92.2\%} accuracy and \textbf{96.9\%} AUC-ROC, positioning itself as an intermediate between the Decision Tree and XGBoost.

\textbf{Network Architecture:}
\begin{itemize}
    \item \textbf{Hidden Layers}: [100, 50, 25 neurons];
    \item \textbf{Activation Function}: ReLU (Rectified Linear Unit);
    \item \textbf{Optimizer}: Adam (learning\_\allowbreak rate=0.001);
    \item \textbf{Regularization}: L2 (alpha=0.0001) to prevent overfitting;
    \item \textbf{Batch Size}: 32 samples;
    \item \textbf{Early Stopping}: Patience of 10 epochs without improvement.
\end{itemize}

\textbf{Training Process:}
The model converged in \textbf{40 epochs} with a final loss of \textbf{0.0852}, demonstrating good generalization capacity. The learning curve revealed stabilization after 30 epochs, without signs of severe \textit{overfitting}. The use of StandardScaler for feature normalization was crucial to accelerate convergence and improve performance.

\textbf{Performance Analysis:}
With \textbf{Precision of 89\%} and \textbf{Recall of 88\%}, the neural network presented balanced performance, significantly reducing false positives compared to the Decision Tree (21\% $\rightarrow$ 11\%). The F1-Score of \textbf{88\%} confirms the balance between precision and recall, making it suitable for scenarios where the cost of false alarms is relevant.

The ability to model non-linear interactions allowed the network to capture more subtle patterns, such as the interaction between \textbf{academic performance} and \textbf{socioeconomic factors}, which the Decision Tree cannot adequately represent.

\textbf{Advantages}: High non-linear modeling capacity, robust performance, good generalization with adequate regularization.

\textbf{Limitations}: Lower interpretability (\textit{black box}), higher computational cost in training, requires data normalization and careful hyperparameter tuning.

\subsection{Detailed Performance of XGBoost Model}

The \textbf{XGBoost} model demonstrated predictive superiority, reaching \textbf{94.3\%} accuracy and \textbf{98.0\%} AUC-ROC, indicating excellent distinction capacity between dropouts and non-dropouts. This result represents a gain of \textbf{6.2 percentage points} over the Decision Tree and \textbf{2.1 percentage points} over the Neural Network.

\textbf{Hyperparameters Used:}
\begin{itemize}
    \item n\_\allowbreak estimators=300: Number of trees in the ensemble;
    \item max\_\allowbreak depth=8: Maximum depth of each tree;
    \item learning\_\allowbreak rate=0.05: Learning rate for regularization;
    \item subsample=0.8: Fraction of samples for each tree;
    \item colsample\_\allowbreak bytree=0.8: Fraction of features for each tree;
    \item min\_\allowbreak child\_\allowbreak weight=3: Minimum weight required for partition;
    \item scale\_\allowbreak pos\_\allowbreak weight: Automatic adjustment for class balancing.
\end{itemize}

\textbf{Training Process:}
The algorithm builds trees sequentially, where each new tree corrects the errors of previous ones through \textit{gradient boosting}. L1 and L2 regularization prevent \textit{overfitting}, while \textit{early stopping} monitors performance on the validation set. Training converged in approximately 180 iterations (out of 300 maximum), indicating good generalization capacity.

The \textbf{Confusion Matrix} (Figure~\ref{fig:matriz_confusao}) reveals that, of the real dropout cases (\textbf{Recall}), \textbf{92\%} were correctly identified. The precision of \textbf{91\%} in the "Dropout" class ensures that the majority of students classified as high risk are, in fact, candidates for dropout.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{Matriz_Confusao_XGBoost_Enriquecido.png}
    \caption{Confusion Matrix of the XGBoost Model with Enriched Data.}
    \label{fig:matriz_confusao}
\end{figure}

The \textbf{ROC Curve} (Figure~\ref{fig:curva_roc}) confirms the high discriminatory capacity, with the area under the curve (\textbf{AUC}) close to \textbf{1.0}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{Curva_ROC_XGBoost_Enriquecido.png}
    \caption{ROC Curve of the XGBoost Model.}
    \label{fig:curva_roc}
\end{figure}

\subsection{Feature Importance Analysis}
\label{sec:importancia_features}

The analysis of variable importance in the XGBoost model (Figure~\ref{fig:importancia_features}) is crucial for understanding risk factors.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Importancia_Features_XGBoost_Enriquecido.png}
    \caption{Variable Importance in the XGBoost Model with Enriched Data.}
    \label{fig:importancia_features}
\end{figure}

The ten most important variables are:
\begin{itemize}
    \item \textbf{Quantity of Semesters Taken}: \textbf{25.5\%}
    \item \textbf{Average of Previous Grades}: \textbf{8.7\%}
    \item \textbf{Combined Performance (Grades and Attendance)}: \textbf{6.4\%}
    \item \textbf{Percentage Completed}: \textbf{5.3\%}
    \item \textbf{Average of Previous Attendance}: \textbf{4.7\%}
    \item \textbf{Scholarship Possession}: \textbf{4.7\%}
    \item \textbf{Days Since Course Creation} (Enriched Feature): \textbf{4.7\%}
    \item \textbf{Course Modality} (Enriched Feature): \textbf{4.2\%}
    \item \textbf{Student Age}: \textbf{4.0\%}
    \item \textbf{Failure Rate}: \textbf{3.8\%}
\end{itemize}
The result reinforces Tinto's model, where \textbf{academic integration} (\textit{Semesters Taken}, \textit{Grades}, \textit{Failures}) is the strongest predictor. The relevance of \textit{enriched features} demonstrates the importance of institutional factors (\textbf{Modality} and \textbf{Course Maturity}) in abandonment risk.

\subsection{Comparative Analysis of the Three Models}

The systematic comparison of the three algorithms reveals important \textit{trade-offs} between performance, interpretability, and complexity:

\textbf{Predictive Performance:} \textbf{XGBoost} presented clear superiority (94.3\% accuracy, 98.0\% AUC-ROC), followed by the \textbf{Neural Network} (92.2\%, 96.9\%) and \textbf{Decision Tree} (88.1\%, 94.8\%). The gain of \textbf{6.2 percentage points} of XGBoost over the Tree represents a significant reduction in false negatives, critical for not losing students at real risk.

\textbf{Interpretability:} The \textbf{Decision Tree} offers explicit rules ("IF percentual\_\allowbreak cursado < 30\% AND semesters > 4 THEN high risk"), facilitating institutional communication. \textbf{XGBoost} provides quantitative feature importance, allowing understanding of main factors, although without direct rules. The \textbf{Neural Network} is essentially a \textit{black box}, making individual justifications difficult.

\textbf{False Positives vs. False Negatives:} For academic dropout, \textbf{Recall} (identifying dropouts) is more critical than Precision. XGBoost reached \textbf{92\% Recall}, losing only 8\% of real cases, against 12\% for the Tree. The cost of false positives (9\% in XGBoost vs. 21\% in the Tree) is compensated by the reduction of real student losses.

\textbf{Computational Complexity:} The Tree has instant training (<1 min), the Neural Network requires ~5 minutes with GPU, and XGBoost ~3 minutes. For production, all are viable, with real-time inference.

\textbf{Recommendation:} For \textbf{production systems}, \textbf{XGBoost} is preferable due to the balance between superior performance and reasonable interpretability. For \textbf{institutional communication}, the Decision Tree can complement as an explanatory tool, while XGBoost operates the real system.

\subsubsection{Theoretical Validation: Tinto's Model}

The empirical results show strong alignment with Tinto's theoretical integration model~\cite{tinto1975,tinto1987}. The predominance of the variable \textbf{Semesters Taken} (25.5\%) as the main predictor corroborates the thesis that institutional persistence time reflects the degree of \textbf{academic and social integration}. Students who remain for more semesters develop stronger ties with the institution, reducing the probability of abandonment.

Variables of \textbf{academic performance} (grades, attendance, failures), which together represent more than \textbf{40\%} of the total importance, validate the concept of academic integration as a protective factor. The relevance of \textbf{socioeconomic factors} (scholarship: 4.7\%) demonstrates that, although less influential than performance, external conditions significantly impact dropout risk, aligning with national literature~\cite{guimaraes2023}.

\subsection{Exploratory Analysis of Dropout Factors (EDA)}

The exploratory analysis of the complete database (\textbf{94,052 records}) highlighted critical factors for proactive interventions:
\begin{itemize}
    \item \textbf{Dropout by Shift:} The \textbf{Virtual} modality presented a dropout rate of \textbf{50.9\%}, significantly higher than the Afternoon shift (\textbf{34.4\%}).
    \item \textbf{Socioeconomic:} \textbf{Defaulting} students had \textbf{46.1\%} dropout, against \textbf{26.9\%} for those \textbf{With scholarship}, highlighting financial vulnerability as a critical factor.
    \item \textbf{Temporal:} The year \textbf{2022} registered the dropout peak (\textbf{57.0\%}), correlated with post-pandemic effects, with strong recovery in \textbf{2023} (\textbf{18.1\%}).
\end{itemize}

\begin{table}[H]
\centering
\caption{Main risk factors for academic dropout identified.}\label{tab:fatores_risco}
\begin{tabularx}{\linewidth}{|l|c|X|}
    \hline % Top line
    \textbf{Factor} & \textbf{Rate} & \textbf{Observations} \\
    \hline % Line after header
    Virtual Modality & 50.9\% & Highest risk identified \\
    \hline % Horizontal line
    Default & 46.1\% & Critical socioeconomic factor \\
    \hline % Horizontal line
    Male Sex & 37.6\% & vs. 29.2\% female \\
    \hline % Horizontal line
    Peak (2022) & 57.0\% & Correlated to pandemic period \\
    \hline % Final bottom line
\end{tabularx}
\end{table}

\subsection{Study Limitations}

Despite promising results, some limitations must be considered:

\begin{itemize}
    \item \textbf{Temporal Representativeness}: The data covers the 2020-2023 period, including the pandemic context (57\% dropout peak in 2022), which may not reflect normal institutional operation scenarios.
    \item \textbf{Missing Variables}: Important psychosocial factors (motivation, social integration, mental health) are not available in the database, limiting the complete capture of the multicausal phenomenon of dropout.
    \item \textbf{Generalization}: The model was trained exclusively with data from PUC Minas, requiring external validation to ensure its applicability in other institutions.
    \item \textbf{XGBoost Interpretability}: Although superior in performance, the XGBoost model is less interpretable than the Decision Tree, hindering the communication of decision criteria to non-technical managers.
\end{itemize}

\subsection{Practical Implications for Academic Management}

The results enable targeted interventions:

\begin{itemize}
    \item \textbf{Early Warning System}: Implementation of a monitoring dashboard that identifies high-risk students from the third semester, a critical period identified in the analysis.
    \item \textbf{Tutoring Programs}: Directing resources to students with low academic performance (grades < 6.0 and attendance < 75\%), especially in virtual courses.
    \item \textbf{Financial Support}: Prioritization of scholarship programs and debt negotiation for defaulting students, given the 46.1\% vs. 26.9\% risk with scholarship.
    \item \textbf{Curriculum Review}: Evaluation of the structure of virtual courses, which present a 50.9\% dropout rate (almost double the general average of 33.2\%).
\end{itemize}

\section{\uppercase{Conclusion}}
\label{sec:conclusion}

This work demonstrated the efficacy and viability of applying \textit{machine learning} for academic dropout prediction at PUC Minas. The analysis of \textbf{94,052 records}, combined with data enrichment, resulted in a high-performance model.

\textbf{Main Conclusions:}
\begin{itemize}
    \item \textbf{Maximum Performance}: The \textbf{XGBoost} model reached \textbf{94.3\%} accuracy and \textbf{98.0\%} AUC-ROC, establishing a robust standard for identifying at-risk students.
    \item \textbf{Feature Relevance}: Importance analysis confirmed that \textbf{academic integration} (time taken, grades) is the dominant predictor (\textbf{25.5\%} for Semesters Taken), followed by \textbf{socioeconomic conditions} and \textbf{institutional characteristics} (\textbf{modality}, \textbf{course maturity}).
    \item \textbf{Theoretical Validation}: Empirical results confirm Tinto's model, demonstrating that academic integration is the most relevant protective factor against dropout.
    \item \textbf{Insights for Intervention}: The EDA identified \textbf{Virtual Modality} and \textbf{Financial Default} as the areas of greatest risk, allowing the institution to direct retention resources more precisely.
    \item \textbf{Data Enrichment}: Integration with institutional data (\textbf{96\% coverage}) added relevant features (modality: 4.2\%, dias\_\allowbreak desde\_\allowbreak criacao: 4.7\%), expanding the model's predictive capacity.
\end{itemize}

The results validate the proposed methodology and provide a high-value predictive tool for academic management.

\textbf{Code Availability:}
All source codes, analysis scripts, and documentation are publicly available in this project's GitHub repository\footnote{\url{https://github.com/VithoriaS/university-dropout-brazil}}, facilitating reproducibility of results and allowing adaptations for other higher education institutions.

\subsection{Future Work}

To improve and expand this study, the following directions are recommended:

\begin{itemize}
    \item \textbf{Incorporation of Psychosocial Variables}: Integration of qualitative data on motivation, expectations, and social integration of students.
    \item \textbf{Longitudinal Temporal Analysis}: Development of sequential models (LSTM, GRU) that capture the temporal trajectory of students over semesters.
    \item \textbf{External Validation}: Testing the model in other higher education institutions to evaluate its generalization.
    \item \textbf{Recommendation System}: Developing a system that not only identifies at-risk students but also recommends personalized interventions based on individual profile.
    \item \textbf{Cost-Benefit Analysis}: Evaluating the financial and academic impact of interventions based on the predictive model.
\end{itemize}

\section*{Acknowledgments}

In compliance with institutional regulations, the authors declare the use of Generative AI tools (Large Language Models) to assist in software development, debugging, manuscript translation, and \LaTeX\ formatting. The scientific conception, data analysis, and conclusions remain the sole responsibility of the authors. The authors also thank the Pontifical Catholic University of Minas Gerais (PUC Minas) for the support provided during this research.

\begin{thebibliography}{}

% --- NEW INTERNATIONAL REFERENCES ADDED ---

\bibitem[Chen and Guestrin, 2016]{chen2016}
Chen, T. and Guestrin, C. (2016).
\newblock Xgboost: A scalable tree boosting system.
\newblock In {\em Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining}, pages 785--794.

\bibitem[Chawla et al., 2002]{chawla2002}
Chawla, N.~V., Bowyer, K.~W., Hall, L.~O., and Kegelmeyer, W.~P. (2002).
\newblock SMOTE: synthetic minority over-sampling technique.
\newblock {\em Journal of artificial intelligence research}, 16:321--357.

\bibitem[Gašević et al., 2016]{gasevic2016}
Gašević, D., Dawson, S., Rogers, T., and Gasevic, D. (2016).
\newblock Learning analytics should not ignore instructional conditions.
\newblock {\em Computers \& Education}, 95:53--72.

\bibitem[Rastrollo-Guerrero et al., 2020]{rastrollo2020}
Rastrollo-Guerrero, J.~L., G{\'o}mez-Pulido, J.~A., and Dur{\'a}n-Dom{\'\i}nguez, A. (2020).
\newblock Analyzing and predicting students' performance by means of machine learning: A review.
\newblock {\em Applied sciences}, 10(3):1042.

% --- YOUR ORIGINAL REFERENCES (National) ---

\bibitem[Pereira and Vieira, 2023]{pereira2023}
Pereira, R. T. and Vieira, R. A. (2023).
\newblock Mineração de dados educacionais para previsão de evasão em cursos superiores.
\newblock {\em Revista de Tecnologias Educacionais em Observação (RTEO)}, 9(2).

\bibitem[Rodrigues and Santos, 2022]{rodrigues2022}
Rodrigues, A. C. C. and Santos, T. D. (2022).
\newblock Modelo preditivo para identificação da evasão escolar no ensino superior: uma revisão sistemática.
\newblock {\em Psicologia: Teoria e Prática}, 24(3).

\bibitem[Carvalho et al., 2021]{carvalho2021}
Carvalho, R. S., Maciel, A. K. A., and Castro, C. L. (2021).
\newblock Acompanhamento discente com base em análise preditiva.
\newblock {\em Revista Brasileira de Informática na Educação}, 29.

\bibitem[Tinto, 1993]{tinto1975}
Tinto, V. (1993).
\newblock {\em Leaving College: Rethinking the Causes and Cures of Student Attrition}.
\newblock University of Chicago Press, Chicago, 2nd edition.

\bibitem[Tinto, 1987]{tinto1987}
Tinto, V. (1987).
\newblock Stages of student departure: reflections on the longitudinal character of student leaving.
\newblock {\em The Journal of Higher Education}, 58(6):627--639.

\bibitem[Batista and Oliveira, 2021]{batista2021}
Batista, G. D. and Oliveira, M. A. (2021).
\newblock Estudo de algoritmos de aprendizado de máquina para predição da evasão escolar.
\newblock In {\em Anais do 32º Simpósio Brasileiro de Informática na Educação (SBIE)}. SBC.

\bibitem[Guimarães et al., 2023]{guimaraes2023}
Guimarães, C. C. et al. (2023).
\newblock A evasão escolar no ensino superior: reflexões e desafios.
\newblock {\em Ensaio: Avaliação e Políticas Públicas em Educação}, 31(120).

\bibitem[Zhao et al., 2023]{zhao2023}
Zhao, L., Wang, Y., and Yang, M. (2023).
\newblock Analysis of college students' dropout behavior based on ensemble learning algorithm.
\newblock {\em IEEE Access}, 11:93627--93637.

\bibitem[Sun and Yang, 2023]{sun2023}
Sun, G. and Yang, L. (2023).
\newblock Prediction of student dropout using deep learning: a case study in online education.
\newblock {\em IEEE Access}, 11:74754--74765.

\bibitem[Semesp, 2024]{semesp2024}
Semesp (2024).
\newblock Mapa do Ensino Superior no Brasil 2024.
\newblock \url{https://www.semesp.org.br/mapa-do-ensino-superior}.

\bibitem[INEP, 2023]{inep2023}
INEP (2023).
\newblock Censo da Educação Superior 2023.
\newblock Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira.

\bibitem[Putra et al., 2025]{putra2025}
Putra, L. G. R. et al. (2025).
\newblock Student dropout prediction using Random Forest and XGBoost.
\newblock {\em INTENSIF}, 9(1).

\bibitem[Liu et al., 2022]{liu2022}
Liu, A. J. et al. (2022).
\newblock Performance and interpretability comparisons of supervised machine learning algorithms: an empirical study.
\newblock {\em IEEE Access}, 10.

\end{thebibliography}

\end{document}